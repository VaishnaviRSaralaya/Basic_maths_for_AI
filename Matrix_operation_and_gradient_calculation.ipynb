{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ba3ced-8e6a-4739-aeb6-7f23ec813bc9",
   "metadata": {},
   "source": [
    "1. Matrix Operations\n",
    "   \n",
    "What is a Matrix?\n",
    "A matrix is a rectangular array of numbers arranged in rows and columns. Matrices are fundamental in linear algebra, and they are widely used in AI, ML, and data science.\n",
    "\n",
    "Basic Matrix Operations\n",
    "Addition & Subtraction\n",
    "If 𝐴and 𝐵 are matrices of the same size: 𝐶=𝐴+𝐵,𝐷=A−𝐵\n",
    "\n",
    "Scalar Multiplication\n",
    "Each element of the matrix is multiplied by a constant 𝑘:𝐵=𝑘𝐴\n",
    "\n",
    "Matrix Multiplication\n",
    "If 𝐴 has dimensions 𝑚×𝑛 and 𝐵 has 𝑛×𝑝, their product 𝐶 is:𝐶=𝐴×𝐵\n",
    "\n",
    "Transpose of a Matrix\n",
    "The transpose of 𝐴 is obtained by flipping its rows and columns:𝐴^𝑇\n",
    " \n",
    "Determinant & Inverse of a Matrix\n",
    "A matrix is invertible if its determinant is non-zero.The inverse is given by: \n",
    "\n",
    "𝐴^(−1)=(1/det(𝐴))*adj(A)\n",
    "\n",
    "Eigenvalues and Eigenvectors\n",
    "\n",
    "An eigenvector of a matrix is a vector that, when multiplied by the matrix, does not change its direction, only its magnitude.\n",
    "\n",
    "The eigenvalue is the factor by which the eigenvector is scaled.\n",
    "\n",
    "Mathematically, if A is a square matrix and v is a vector, then:\n",
    "\n",
    "𝐴𝑣=𝜆𝑣\n",
    "\n",
    "where:\n",
    "\n",
    "𝑣 is the eigenvector,\n",
    "𝜆(lambda) is the eigenvalue and\n",
    "𝐴 is the given matrix\n",
    "\n",
    "  \n",
    "2. Gradient Calculations\n",
    "\n",
    " What is a Gradient?\n",
    "The gradient of a function represents the direction and rate of the steepest increase. It is a key concept in calculus, optimization, and deep learning.\n",
    "\n",
    "For a function \n",
    "𝑓(𝑥,𝑦), the gradient is a vector of partial derivatives:\n",
    "\n",
    "∇𝑓=(∂𝑓/∂𝑥,∂𝑓/∂𝑦) (This is symbolic differentiation)\n",
    "\n",
    "In machine learning, the gradient is used in gradient descent, where model parameters are updated iteratively:\n",
    "\n",
    "𝑤=𝑤−𝛼∇𝑓(𝑤)\n",
    "\n",
    "where:\n",
    "\n",
    "𝑤= parameters (weights)\n",
    "𝛼= learning rate\n",
    "∇𝑓(𝑤)= gradient of the loss function\n",
    "\n",
    "Numerical Gradient Approximation\n",
    "When an analytical gradient is difficult to compute, we approximate it using the finite difference method:\n",
    "\n",
    "𝑓′(𝑥) ≈ (𝑓(𝑥+ℎ)−𝑓(𝑥−ℎ))/2ℎ,ℎ is a small step size (e.g., ℎ=1𝑒−5=10^(-5)) (This is numerical  differentiation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899af0a5-7d7f-4386-918f-80441eb4bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[99 16]\n",
      " [ 1 30]]\n",
      "\n",
      "Matrix Subtraction:\n",
      " [[-75 -30]\n",
      " [  9   8]]\n",
      "\n",
      "Matrix Multiplication:\n",
      " [[1072  199]\n",
      " [ 359  324]]\n",
      "\n",
      "Determinant of A: 263.00000000000006\n",
      "\n",
      "Inverse of A:\n",
      " [[ 0.07224335  0.02661597]\n",
      " [-0.01901141  0.04562738]]\n",
      "\n",
      "Eigenvalues of A: [15.5+4.76969601j 15.5-4.76969601j]\n",
      "\n",
      "Eigenvectors of A:\n",
      " [[ 0.76376262+0.j         0.76376262-0.j       ]\n",
      " [-0.38188131-0.5204165j -0.38188131+0.5204165j]]\n"
     ]
    }
   ],
   "source": [
    "#Matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# Define two complex matrices\n",
    "A = np.array([[12, -7], [5, 19]])\n",
    "B = np.array([[87, 23], [-4, 11]])\n",
    "\n",
    "# Matrix Addition\n",
    "addition = A + B\n",
    "print(\"Matrix Addition:\\n\", addition)\n",
    "\n",
    "# Matrix Subtraction\n",
    "subtraction = A - B\n",
    "print(\"\\nMatrix Subtraction:\\n\", subtraction)\n",
    "\n",
    "# Matrix Multiplication (Dot Product)\n",
    "multiplication = np.dot(A, B)\n",
    "print(\"\\nMatrix Multiplication:\\n\", multiplication)\n",
    "\n",
    "# Determinant of A\n",
    "det_A = np.linalg.det(A)\n",
    "print(\"\\nDeterminant of A:\", det_A)\n",
    "\n",
    "# Inverse of A\n",
    "if det_A != 0:  # Check if the determinant is non-zero\n",
    "    inverse_A = np.linalg.inv(A)\n",
    "    print(\"\\nInverse of A:\\n\", inverse_A)\n",
    "else:\n",
    "    print(\"\\nMatrix A is singular and cannot be inverted.\")\n",
    "\n",
    "# Eigenvalues and Eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(\"\\nEigenvalues of A:\", eigenvalues)\n",
    "print(\"\\nEigenvectors of A:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee136587-7337-4dd8-a7cb-d16d01e129c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient of f:\n",
      "∂f/∂x = 14*x - 8*y - 41\n",
      "∂f/∂y = -8*x + 26*y\n",
      "\n",
      "Gradient at (x=1, y=2):\n",
      "∂f/∂x = -43.0000000000000, ∂f/∂y = 44.0000000000000\n"
     ]
    }
   ],
   "source": [
    "# This program calculates gradients (derivatives) of a function using symbolic differentiation.\n",
    "\n",
    "import sympy as sp\n",
    "'''SymPy is a Python library for symbolic mathematics. Unlike NumPy, which deals with numerical computations, SymPy performs symbolic calculations, meaning it manipulates mathematical expressions just like you would on paper.\n",
    "\n",
    "# Define the variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function f(x, y)\n",
    "f = 7*x**2 + 13*y**2 -8*x*y - 41*x + 59\n",
    "\n",
    "# Compute the gradient (partial derivatives)\n",
    "grad_x = sp.diff(f, x)  # Partial derivative w.r.t x\n",
    "grad_y = sp.diff(f, y)  # Partial derivative w.r.t y\n",
    "\n",
    "print(\"\\nGradient of f:\")\n",
    "print(\"∂f/∂x =\", grad_x)\n",
    "print(\"∂f/∂y =\", grad_y)\n",
    "\n",
    "# Evaluate the gradient at (x=1, y=2)\n",
    "grad_at_point = {x: 1, y: 2}\n",
    "grad_x_val = grad_x.evalf(subs=grad_at_point)\n",
    "grad_y_val = grad_y.evalf(subs=grad_at_point)\n",
    "\n",
    "print(\"\\nGradient at (x=1, y=2):\")\n",
    "print(f\"∂f/∂x = {grad_x_val}, ∂f/∂y = {grad_y_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d935e079-7baa-4dd1-889d-870b5cd83835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical Gradient at x=2: 12.000000001677334\n"
     ]
    }
   ],
   "source": [
    "# functions where symbolic differentiation is hard, we use numerical differentiation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define function\n",
    "def f(x):\n",
    "    return 12*x**3 - 41*x**2 + 32*x + 19  # Example function\n",
    "\n",
    "# Numerical derivative using finite differences\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Compute gradient at x=2\n",
    "grad_value = numerical_derivative(f, 2)\n",
    "print(\"\\nNumerical Gradient at x=2:\", grad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8ee50-bf52-4cc8-be8c-a4ac5bb8439a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
